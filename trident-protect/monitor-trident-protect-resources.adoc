---
permalink: trident-protect/monitor-trident-protect-resources.html 
sidebar: sidebar 
keywords: manage, authentication, rbac 
summary: È possibile monitorare lo stato delle risorse Trident Protect utilizzando le metriche dello stato kube e Prometheus. In questo modo otterrai informazioni sullo stato di salute relative a implementazioni, nodi e pod. 
---
= Monitorare le risorse Trident Protect
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
È possibile utilizzare gli strumenti open source kube-state-metrics, Prometheus e Alertmanager per monitorare lo stato delle risorse protette da Trident Protect.

Il servizio di metriche dello stato di kube genera metriche dalla comunicazione delle API Kubernetes. Il suo utilizzo con Trident Protect espone informazioni utili sullo stato delle risorse nell'ambiente.

Prometheus è un toolkit in grado di acquisire i dati generati da metriche dello stato di kube e presentarli come informazioni facilmente leggibili su questi oggetti. Insieme, le metriche kube-state e Prometheus vi offrono un modo per monitorare lo stato e lo stato delle risorse che state gestendo con Trident Protect.

Alertmanager è un servizio che acquisisce gli avvisi inviati da strumenti come Prometheus e li indirizza alle destinazioni configurate dall'utente.

[NOTE]
====
Le configurazioni e le istruzioni incluse in questa procedura sono solo esempi; è necessario personalizzarle in base all'ambiente in uso. Per istruzioni specifiche e assistenza, consultare la seguente documentazione ufficiale:

* https://github.com/kubernetes/kube-state-metrics/tree/main["documentazione kube-state-metrics"^]
* https://prometheus.io/docs/introduction/overview/["Documentazione Prometheus"^]
* https://github.com/prometheus/alertmanager["Documentazione di Alertmanager"^]


====


== Fase 1: Installare gli strumenti di monitoraggio

Per abilitare il monitoraggio delle risorse in Trident Protect, è necessario installare e configurare le metriche di stato kube, Prometus e Alertmanager.



=== Installa metriche-stato-kube

È possibile installare parametri kube-state-metrics utilizzando Helm.

.Fasi
. Aggiungere il grafico Helm kube-state-metrics. Ad esempio:
+
[source, console]
----
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
----
. Creare un file di configurazione per il grafico Helm (ad esempio, `metrics-config.yaml`). È possibile personalizzare la seguente configurazione di esempio in base all'ambiente in uso:
+
.Metrics-config.yaml: Configurazione del grafico Helm kube-state-metrics
[source, yaml]
----
---
extraArgs:
  # Collect only custom metrics
  - --custom-resource-state-only=true

customResourceState:
  enabled: true
  config:
    kind: CustomResourceStateMetrics
    spec:
      resources:
      - groupVersionKind:
          group: protect.trident.netapp.io
          kind: "Backup"
          version: "v1"
        labelsFromPath:
          backup_uid: [metadata, uid]
          backup_name: [metadata, name]
          creation_time: [metadata, creationTimestamp]
        metrics:
        - name: backup_info
          help: "Exposes details about the Backup state"
          each:
            type: Info
            info:
              labelsFromPath:
                appVaultReference: ["spec", "appVaultRef"]
                appReference: ["spec", "applicationRef"]
rbac:
  extraRules:
  - apiGroups: ["protect.trident.netapp.io"]
    resources: ["backups"]
    verbs: ["list", "watch"]

# Collect metrics from all namespaces
namespaces: ""

# Ensure that the metrics are collected by Prometheus
prometheus:
  monitor:
    enabled: true
----
. Installare le metriche di stato kube distribuendo il grafico Helm. Ad esempio:
+
[source, console]
----
helm install custom-resource -f metrics-config.yaml prometheus-community/kube-state-metrics --version 5.21.0
----
. Configurare le metriche dello stato-kube per generare metriche per le risorse personalizzate utilizzate da Trident Protect seguendo le istruzioni riportate nella https://github.com/kubernetes/kube-state-metrics/blob/main/docs/metrics/extend/customresourcestate-metrics.md#custom-resource-state-metrics["documentazione sulle risorse personalizzate kube-state-metrics"^] .




=== Installare Prometheus

È possibile installare Prometheus seguendo le istruzioni riportate nella https://prometheus.io/docs/prometheus/latest/installation/["Documentazione Prometheus"^] .



=== Installare Alertmanager

È possibile installare Alertmanager seguendo le istruzioni riportate nella https://github.com/prometheus/alertmanager?tab=readme-ov-file#install["Documentazione di Alertmanager"^] .



== Fase 2: Configurare gli strumenti di monitoraggio per lavorare insieme

Dopo aver installato gli strumenti di monitoraggio, è necessario configurarli per lavorare insieme.

.Fasi
. Integra metriche-stato-kube con Prometheus. Modificare il file di configurazione di Prometheus (`prometheus.yaml`) e aggiungere le informazioni del servizio kube-state-metrics. Ad esempio:
+
.prometheus.yaml: integrazione del servizio kube-state-metrics con Prometheus
[source, yaml]
----
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: trident-protect
data:
  prometheus.yaml: |
    global:
      scrape_interval: 15s
    scrape_configs:
      - job_name: 'kube-state-metrics'
        static_configs:
          - targets: ['kube-state-metrics.trident-protect.svc:8080']
----
. Configurare Prometheus per instradare gli avvisi ad Alertmanager. Modificare il file di configurazione di Prometheus (`prometheus.yaml`) e aggiungere la seguente sezione:
+
.prometheus.yaml: Invia avvisi ad Alertmanager
[source, yaml]
----
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager.trident-protect.svc:9093
----


.Risultato
Prometheus può ora raccogliere le metriche dalle metriche dello stato del kube e inviare avvisi ad Alertmanager. Ora si è pronti a configurare quali condizioni attivano un avviso e dove inviare gli avvisi.



== Passaggio 3: Configurare le destinazioni degli avvisi e degli avvisi

Dopo aver configurato gli strumenti per lavorare insieme, è necessario configurare il tipo di informazioni che attivano gli avvisi e la posizione in cui devono essere inviati.



=== Esempio di avviso: Errore di backup

Nell'esempio seguente viene definito un avviso critico che viene attivato quando lo stato della risorsa personalizzata di backup è impostato su `Error` per 5 secondi o più. È possibile personalizzare questo esempio in base all'ambiente in uso e includere questo frammento YAML nel `prometheus.yaml` file di configurazione:

.rules.yaml: Definisci un avviso Prometheus per i backup non riusciti
[source, yaml]
----
rules.yaml: |
  groups:
    - name: fail-backup
        rules:
          - alert: BackupFailed
            expr: kube_customresource_backup_info{status="Error"}
            for: 5s
            labels:
              severity: critical
            annotations:
              summary: "Backup failed"
              description: "A backup has failed."
----


=== Configurare Alertmanager per inviare avvisi ad altri canali

È possibile configurare Alertmanager in modo che invii notifiche ad altri canali, quali e-mail, PagerDuty, Microsoft Teams o altri servizi di notifica specificando la rispettiva configurazione nel `alertmanager.yaml` file.

Nell'esempio seguente, Alertmanager configura l'invio di notifiche a un canale Slack. Per personalizzare questo esempio in base all'ambiente in uso, sostituire il valore della `api_url` chiave con l'URL slack webhook utilizzato nell'ambiente in uso:

.alertmanager.yaml: invia avvisi a un canale Slack
[source, yaml]
----
data:
  alertmanager.yaml: |
    global:
      resolve_timeout: 5m
    route:
      receiver: 'slack-notifications'
    receivers:
      - name: 'slack-notifications'
        slack_configs:
          - api_url: '<your-slack-webhook-url>'
            channel: '#failed-backups-channel'
            send_resolved: false
----